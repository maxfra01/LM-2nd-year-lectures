# Logistic regression

Loss of logistic regression, casted as a cost function:

![[Pasted image 20241015161840.png]]

Where $\sigma(x) = \frac{1}{1+e^{-x}}$ is the **sigmoid function**.
This is called **logistic loss**, that is different from the **hinge loss** (used in SVM).
To minimize the loss we need to apply the first-order condition:
$$
\nabla_{\Theta}l_{\Theta} = 0
$$
![[Pasted image 20241015162657.png]]

We now focus on this term (3 concatenated functions) and if we solve this we get a solution depending on both $a$ and $b$. We need to do calculate all derivatives w.r.t. all variables.
We notice that **the parameters enter the gradient in a non linear way**.
Thus $\nabla_{\Theta}l_{\Theta}= 0$ is not a linear system and it is also a **transcendental** equation (no analytical solution).
We need to find another approach to optimize our loss.

### Gradient descent

Our approach will be **gradient descent**: looking at the **loss landscape** we iteratively move in the direction where the function decreases the most.

![[Pasted image 20241015163611.png]]

In other words we compute at each time step $t$:
$$
\Theta^{(t+1)} = \Theta^{(t)} - \alpha \cdot \nabla l_{\Theta (t)}
$$
This approach requires that **function is differentiable**.
Also, gradient descent **may remain stuck in stationary point**, but they are not local minima.
The $\alpha$ parameter is called **learning rate**: being able to select this parameter in an accurate way may lead to efficiency.
- If $\alpha$ is small we will have slow convergence
- If $\alpha$ is too big we will have overshooting
- Selecting the correct $\alpha$ allows us to reach the local minimum in a certain time

The learning rate can be **adaptive**:
1. We can decrease $\alpha$ to a **decay parameter** $\rho$
2. We can accumulate past gradients and keep moving in their direction ![[Pasted image 20241015165403.png]]With the first equation we are accumulating the gradient at each step (momentum), then with the second equation we are moving in a direction. The more the gradient is pointing to a certain direction, the faster we will move.

We can also apply gradient descent to non-convex problem, but there are no guarantee that we find a minimum. However, to gain generalization, we are rarely interested in a global optimum. (it means overfit the model)
We are more interested in local optimum, more efficient and computationally feasible.

### Stochastic gradient descend

There are limitations to classic gradient descend: the number of samples and the number of parameters can be very high. To avoid this problem we can use a **mini-batch** extracted from the training sample to approximate the gradient. Even if the mini-batch is small, it should be representative for our data.
After applying this approach, the gradient is approximated, but with a **significantly speedup**.

We can summarize the algorithm in the following steps:
1. Initialize $\theta$
2. Pick a mini-batch $\mathcal{B}$
3. Update with the downhill step $\theta \leftarrow \theta - \alpha \nabla l_{\theta}(\mathcal{B})$ 
4. Go back to step 2
Once you cover the entire training set we have an **epoch**.

Pros of this approach: Oscillations introduces randomness that allows us to avoid overfitting and also avoid local minimum.
Also, running the algorithm for different epochs avoid stopping in a local minimum.

Practical consideration with SGD:
- We need to shuffle data to avoid data bias
- Each mini-batch can be processed in parallel
- If a dataset is large, we will have initial progress and then a slow asymptotic convergence
- Small mini-batches can offer a **regularization** effect.

# Neural networks

### Perceptron

Basic component: **perceptron**, which is a single component that acts as a weighted sum of inputs and weights, then a "step function" is applied.
If we call $w$ the weights vector and $x$ the inputs, we can describe the perceptron as:
$$
h(x_{i}) =\text{sign}(w^Tx_{i}+b)
$$
Perceptron can be used as classifier with the following algorithm:
1. Start with weights = 0
2. For each training instance we classify a sample
3. If we are correct we do nothing, otherwise we update the weights using this formula: $w=w+y^* \cdot x$
If data are linearly separable, then the perceptron will commit at most $\frac{1}{\gamma^2}$ mistakes, where $\gamma$ is the "margin"

![[Pasted image 20241015175613.png]]

Problems with perceptron:
- poor generalization
- overfitting
- If data are not linearly separable, then we get trash

To **improve the perceptron** we could add a non-linearity to provide more "probabilistic" output: for example, instead of the step function we could use a sigmoid.

### Deep ReLU network

Stacking ReLU function as the output of layers, we can approximate any complicated function, to model real world.

This concept above is generalized in the following theorem:

![[Pasted image 20241015182534.png]]

These type of theorem are not constructive, i.e. they do not tell us how to find these approximation, but they guarantee the existence.

### Training

In general we consider the MSE as the loss for the training process. However the loss in not convex w.r.t. the parameters $\Theta$. We can identify special cases:
- If we have one layer, no activation, MSE as loss -> Linear regression
- If we have one layer. sigmoid activation, logistic loss -> Logistic regression

In order to optimize this model we need to compute gradients, but computationally that is non feasible. So we need another strategy, called **backpropagation**.










